{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb12824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from model_function.ipynb\n",
      "CPU times: total: 1.3 s\n",
      "Wall time: 1.53 s\n",
      "CPU times: total: 2.33 s\n",
      "Wall time: 2.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import time\n",
    "from model_function import read_data, data_prep, decision_tree, kn_classifier, rf_classifier, log_model, model_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a458b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, ytest_data = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470d993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = data_prep(train_data, test_data, ytest_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9746705",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638bfb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = decision_tree(X_train,y_train,X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521b7e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score = 100.0 %\n",
      "Precision score = 100.0 %\n",
      "classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    127037\n",
      "\n",
      "    accuracy                           1.00    127037\n",
      "   macro avg       1.00      1.00      1.00    127037\n",
      "weighted avg       1.00      1.00      1.00    127037\n",
      "\n",
      "Confusion Matrix\n",
      "[[127037]]\n",
      "Root Mean Squared Error (RMSE) = 0.0\n",
      "Recall score = 1.0\n",
      "F1 score =  1.0\n"
     ]
    }
   ],
   "source": [
    "acc, prc,clf,conf,rmse,recall,f1 = model_diag(y_test,pred)\n",
    "print(\"Accuracy score =\", (round(acc,2)*100),\"%\")\n",
    "print(\"Precision score =\", (round(prc,2)*100),\"%\")\n",
    "print(\"classification Report\")\n",
    "print(clf)\n",
    "print(\"Confusion Matrix\")\n",
    "print(conf)\n",
    "print(\"Root Mean Squared Error (RMSE) =\",round(rmse,2))\n",
    "print(\"Recall score =\", recall)\n",
    "print(\"F1 score = \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37ef2d",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c5fdf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = kn_classifier(X_train,y_train,X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bc59ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score = 100.0 %\n",
      "Precision score = 0.0 %\n",
      "classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    127037\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00    127037\n",
      "   macro avg       0.50      0.50      0.50    127037\n",
      "weighted avg       1.00      1.00      1.00    127037\n",
      "\n",
      "Confusion Matrix\n",
      "[[126695    342]\n",
      " [     0      0]]\n",
      "Root Mean Squared Error (RMSE) = 0.05\n",
      "Recall score = 0.9986539354676196\n",
      "F1 score =  0.49932606056784323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc, prc,clf,conf,rmse,recall,f1 = model_diag(y_test,pred)\n",
    "print(\"Accuracy score =\", (round(acc,2)*100),\"%\")\n",
    "print(\"Precision score =\", (round(prc,2)*100),\"%\")\n",
    "print(\"classification Report\")\n",
    "print(clf)\n",
    "print(\"Confusion Matrix\")\n",
    "print(conf)\n",
    "print(\"Root Mean Squared Error (RMSE) =\",round(rmse,2))\n",
    "print(\"Recall score =\", recall)\n",
    "print(\"F1 score = \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5c76c",
   "metadata": {},
   "source": [
    "## RandomForestClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba4f89a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = rf_classifier(X_train,y_train,X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0972cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score = 100.0 %\n",
      "Precision score = 100.0 %\n",
      "classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    127037\n",
      "\n",
      "    accuracy                           1.00    127037\n",
      "   macro avg       1.00      1.00      1.00    127037\n",
      "weighted avg       1.00      1.00      1.00    127037\n",
      "\n",
      "Confusion Matrix\n",
      "[[127037]]\n",
      "Root Mean Squared Error (RMSE) = 0.0\n",
      "Recall score = 1.0\n",
      "F1 score =  1.0\n"
     ]
    }
   ],
   "source": [
    "acc, prc,clf,conf,rmse,recall,f1 = model_diag(y_test,pred)\n",
    "print(\"Accuracy score =\", (round(acc,2)*100),\"%\")\n",
    "print(\"Precision score =\", (round(prc,2)*100),\"%\")\n",
    "print(\"classification Report\")\n",
    "print(clf)\n",
    "print(\"Confusion Matrix\")\n",
    "print(conf)\n",
    "print(\"Root Mean Squared Error (RMSE) =\",round(rmse,2))\n",
    "print(\"Recall score =\", recall)\n",
    "print(\"F1 score = \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6739c3",
   "metadata": {},
   "source": [
    "## LogisticRegression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "821d5884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = log_model(X_train,y_train,X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47be1aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score = 99.0 %\n",
      "Precision score = 0.0 %\n",
      "classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    127037\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99    127037\n",
      "   macro avg       0.50      0.50      0.50    127037\n",
      "weighted avg       1.00      0.99      1.00    127037\n",
      "\n",
      "Confusion Matrix\n",
      "[[125851   1186]\n",
      " [     0      0]]\n",
      "Root Mean Squared Error (RMSE) = 0.1\n",
      "Recall score = 0.9953320686099325\n",
      "F1 score =  0.4976550884185885\n"
     ]
    }
   ],
   "source": [
    "acc, prc,clf,conf,rmse,recall,f1 = model_diag(y_test,pred)\n",
    "print(\"Accuracy score =\", (round(acc,2)*100),\"%\")\n",
    "print(\"Precision score =\", (round(prc,2)*100),\"%\")\n",
    "print(\"classification Report\")\n",
    "print(clf)\n",
    "print(\"Confusion Matrix\")\n",
    "print(conf)\n",
    "print(\"Root Mean Squared Error (RMSE) =\",round(rmse,2))\n",
    "print(\"Recall score =\", recall)\n",
    "print(\"F1 score = \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da1f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
